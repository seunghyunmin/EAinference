---
title: "Introduction to the EAlasso package"
author: "Seunghyun Min"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to EAlasso}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

EAlasso package is designed to help generating simulation based inference in lasso type estimators. 
The package include

- Gaussian bootstrap and wild miltiple bootstrap
- Imporance sampler
- Metropolis-Hastings sampler for lasso estimator
- Post-selection inference for lasso estimator


## Loss function for four types of lasso estimators
Let the unknown true model is 
$$ y = X\beta_0 + \epsilon,$$
where $\beta_0$ is unknown true coefficient and $\epsilon_i \sim_{iid} N(0,\sigma^2)$. 

We use following loss functions. 
#$$ \ell_{lasso}(\beta ; \lambda) = \frac{1}{2}||y-X\beta||^2_2 + n\lambda\sum_i w_i|\beta_i|$$
$$ \ell_{grlasso}(\beta ; \lambda) = \frac{1}{2}||y-X\beta||^2_2 + n\lambda\sum_j w_j||\beta_{(j)}||_2,$$
#$$ \ell_{slasso}(\beta, \sigma ; \lambda) = \frac{1}{2\sigma}||y-X\beta||^2_2 + \frac{\sigma}{2} + n\lambda\sum_i w_i|\beta_i|$$
$$ \ell_{sgrlasso}(\beta, \sigma ; \lambda) = \frac{1}{2\sigma}||y-X\beta||^2_2 + \frac{\sigma}{2} + n\lambda\sum_j w_j||\beta_{(j)}||_2,$$
where {\it grlasso} and {\it sgrlasso} represent group lasso and scaled group lasso, respectively,
and $w_j$ is the given weight factor. 

Loss functions for lasso and scaled lasso can be treated as special cases of
group lasso and group scaled lasso when the group size is one, respectively.

We use R package: gglasso to compute lasso-type estimators.

## Parametric Bootstrap

PBsampler function supports two bootstrap methods; Gaussian bootstrap and wild multiplier bootstrap. 
Due to the fact that the sampling distirbution of $(\hat{\beta}, S)$, coefficient estimator and its subgradient, is characterized by $(\beta_0, \sigma^2, \lambda)$, users require to provide PE(either the estimate of $\beta_0$ or the estimate of $E(y) = X\beta_0$), sig2(or estimate of $\sigma^2$)
and lbd(\lambda from above loss functions).

By specifying two set of arguments,
(PE_1, sig2_1, lbd_1) and (PE_2, sig2_2, lbd_2), one can sample from mixtrue distribution.
In this way, samples will be drawn from (PE_1, sig2_1, lbd_1) with 1/2 probability and from 
(PE_2, sig2_2, lbd_2) with another 1/2 probability. 

```{r}
library(EAlasso)
set.seed(1234)
n <- 5; p <- 10
Niter <-  10
group <- rep(1:(p/5), each = 5)
weights <- rep(1, p/5)
# beta0 <- c(rep(1,5), rep(0, p-5))
X <- matrix(rnorm(n*p), n)
# Y <- X %*% beta0 + rnorm(n)
# TEMP <- Lasso.MHLS(X = X, Y = Y, type = "sgrlasso",
#                    lbd = "cv.1se", group = group,
#                    weights = weights, plot.it = TRUE, verbose= TRUE)
# TEMP <- Lasso.MHLS(X = X, Y = Y, type = "slasso",
#                    lbd = "cv.1se", plot.it = TRUE, verbose= TRUE)
# sig2.hat <- TEMP$sigmaHat^2
# 
# TEMP <- Lasso.MHLS(X = X, Y = Y, type = "sgrlasso",
#                    lbd = "cv.1se", group = group,
#                    weights = weights)
#
# Using non-mixture distribution
#
PBsampler(X = X, PE_1 = rep(0, p), sig2_1 = 1, lbd_1 = .5,
weights = weights, group = group, type = "grlasso", niter = Niter, parallel = FALSE)
#
# Using mixture distribution
#
PBsampler(X = X, PE_1 = rep(0, p), sig2_1 = 1, lbd_1 = .5,
 PE_2 = rep(1, p), sig2_2 = 2, lbd_2 = .3, weights = weights,
 group = group, type = "grlasso", niter = Niter, parallel = FALSE)
```
    
## Importance Sampler for High Dimensional Data
Importance Sampler enables users to draw samples from extreme regions.
By using `hdIS` method, one can easily compute importance weights which is the ratio of 
target density over proposal density. By drawing samples from proposal distribution 
which is denser around target region, more stable estimators can be produced.

Users can simply draw samples from proposal distribution using `PBsampler` and 
plug in the result into `hdIS` with target distribution parameters in order to 
compute the importance weights.
```{r}
set.seed(1234)
n <- 10
p <- 30
Niter <-  5
group <- rep(1:(p/5), each = 5)
weights <- rep(1, p/5)
X <- matrix(rnorm(n*p), n)
# Target distribution parameter
PETarget <- rep(0, p)
sig2Target <- .5
lbdTarget <- .37
## Proposal distribution parameter
PEProp1 <- rep(1, p)
sig2Prop1 <- .5
lbdProp1 <- 1

PB <- PBsampler(X = X, PE_1 = PEProp1, sig2_1 = sig2Prop1,
 lbd_1 = lbdProp1, weights = weights, group = group, niter = Niter,
 type="grlasso", PEtype = "coeff")

hdIS(PB, PETarget = PETarget, sig2Target = sig2Target, lbdTarget = lbdTarget,
log = TRUE)
```

## Metropols-Hastings Lasso Sampler

In this section, we introduce `MHLS` method, a MCMC sampler for lasso estimator. 
Although bootstrap is one of the most convinient sampling methods, it has a clear
limitation which is that sampling from the conditional distribution is impossible. 
MCMC sampler, in such sense, can easily draw samples from the 
conditional distribution. Here, we introduce MHLS function which draws lasso samples 
under the fixed active set, A. 

```{r}
set.seed(123)
n <- 5
p <- 10
X <- matrix(rnorm(n*p),n)
Y <- X %*% rep(1,p) + rnorm(n)
weights <- rep(1,p)
lbd <- .37
LassoResult <- Lasso.MHLS(X = X,Y = Y,lbd = lbd, type = "lasso", weights = weights)
B0 <- LassoResult$B0
S0 <- LassoResult$S0
Result <- MHLS(X = X, PE = B0, sig2 = 1, lbd = 1,
    weights = weights, B0 = B0, S0 = S0, niter = 100, burnin = 0,
    PEtype = "coeff")
Result
```
We provide summary and plot function for MHLS results. 
```{r}
summary(Result)
```
```{r, fig.width = 7, fig.height = 4.5}
plot(Result, index=c(1,4,9))
```

## Post-selection Inference

In this section, we present 'Postinference.MHLS', a method for post-selection 
inference. 

```{r}
Postinference.MHLS(X = X, Y = Y, lbd = lbd, sig2.hat = 1, alpha = .05,
 nChain = 5, niterPerChain = 20, parallel = !(.Platform$OS.type == "windows"))
```


